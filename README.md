# Предсказание оттока клиентов
## Описание проекта
Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.

Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Нам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком.

Источник данных: https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling

## Описание данных
**Признаки**
- RowNumber — индекс строки в данных
- CustomerId — уникальный идентификатор клиента
- Surname — фамилия
- CreditScore — кредитный рейтинг
- Geography — страна проживания
- Gender — пол
- Age — возраст
- Tenure — сколько лет человек является клиентом банка
- Balance — баланс на счёте
- NumOfProducts — количество продуктов банка, используемых клиентом
- HasCrCard — наличие кредитной карты
- IsActiveMember — активность клиента
- EstimatedSalary — предполагаемая зарплата

**Целевой признак**
- Exited — факт ухода клиента

## Общие выводы по проекту
**Задача состояла в подборе наилучшей по значению f1-score модели для прогнозирования ухода клиента из банка в ближайшее время.**

**Перед началом анализа данные были подготовлены:**
- удалены незначимые признаки, т.е. те которые не влияют на таргет (индекс строки, идентификатор клиента, фамилия);
- заполнены пропуски.
- закодированы категориальных признаков (применялся one-hot encoding);
- масштабированы количественных признаков;
- исходные данные разделены на обучающую, валидационную и тестовую выборки.

Мы попробовали 3 различные модели: дерево решений, случайный лес и логистическую регрессию. У каждой модели мы также меняли гиперпараметры, чтобы добиться наиболее высокой оценки качества.

Обучение проводилось как на несбалансированных исходных данных, так и с применением различных методов борьбы с дисбалансом: взвешивание классов, upsampling и downsampling.

**По результатам экспериментов на валидационной выборке лучший результат показала модель случайного леса со следующими гиперпараметрами: максимальная глубина дерева – 12, количество деревьев – 100. Такой результат был достигнут с применением метода взвешивания классов при обучении.**

Затем модель с данными параметрами была обучена на объединенной выборке из обучающей и валидационной и проверена на тестовой выборке.

**В результате итоговая метрика f1-score на тестовой выборке составляет 0.62. Метрика AUC-ROC равна 0.86, соответственно.**
